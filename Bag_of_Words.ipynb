{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request \n",
    "\n",
    "html = urllib.request.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
    "html = html.read()\n",
    "\n",
    "article = bs.BeautifulSoup(html, 'lxml')\n",
    "paragraphs = article.find_all('p')\n",
    "\n",
    "text = ''\n",
    "for paragraph in paragraphs:\n",
    "    text += paragraph.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize to sentences and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of sentences:  47\n",
      "21st sentence:  the cache language models upon which many speech recognition systems now rely are examples of such statistical models \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "sentences = nltk.sent_tokenize(text) # tokenize to sentences\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = sentences[i].lower() # convert to lower case\n",
    "    sentences[i] = re.sub(r'\\W', ' ', sentences[i]) # remove punctuations\n",
    "    sentences[i] = re.sub(r'\\s+', ' ', sentences[i]) # replace multiple spaces with single space\n",
    "\n",
    "print('No. of sentences: ', len(sentences)) \n",
    "print('21st sentence: ', sentences[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize to words, remove stopwords, and create a dictionary of word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word frequencies:  {'natural': 19, 'language': 24, 'processing': 16, 'nlp': 3, 'subfield': 1, 'linguistics': 3, 'computer': 3, 'science': 1, 'information': 3, 'engineering': 1, 'artificial': 2, 'intelligence': 4, 'concerned': 1, 'interactions': 1, 'computers': 2, 'human': 4, 'languages': 2, 'particular': 1, 'program': 1, 'process': 1, 'analyze': 1, 'large': 3, 'amounts': 2, 'data': 10, 'challenges': 1, 'frequently': 2, 'involve': 1, 'speech': 4, 'recognition': 2, 'understanding': 1, 'generation': 1, 'history': 1, 'generally': 3, 'started': 1, '1950s': 1, 'although': 1, 'work': 3, 'found': 2, 'earlier': 1, 'periods': 1, '1950': 1, 'alan': 1, 'turing': 2, 'published': 3, 'article': 1, 'titled': 1, 'computing': 1, 'machinery': 1, 'proposed': 1, 'called': 2, 'test': 1, 'criterion': 1, 'clarification': 1, 'needed': 1, 'georgetown': 1, 'experiment': 1, '1954': 1, 'involved': 1, 'fully': 1, 'automatic': 1, 'translation': 10, 'sixty': 2, 'russian': 1, 'sentences': 1, 'english': 1, 'authors': 1, 'claimed': 1, 'within': 1, 'three': 1, 'five': 1, 'years': 1, 'machine': 16, 'would': 1, 'solved': 1, 'problem': 1, '2': 1, 'however': 6, 'real': 7, 'progress': 1, 'much': 3, 'slower': 1, 'alpac': 1, 'report': 1, '1966': 2, 'ten': 1, 'year': 1, 'long': 1, 'research': 8, 'failed': 1, 'fulfill': 1, 'expectations': 1, 'funding': 1, 'dramatically': 1, 'reduced': 1, 'little': 1, 'conducted': 1, 'late': 3, '1980s': 4, 'first': 2, 'statistical': 9, 'systems': 15, 'developed': 4, 'notably': 1, 'successful': 1, '1960s': 1, 'shrdlu': 1, 'system': 3, 'working': 1, 'restricted': 2, 'blocks': 1, 'worlds': 1, 'vocabularies': 1, 'eliza': 3, 'simulation': 1, 'rogerian': 1, 'psychotherapist': 1, 'written': 4, 'joseph': 1, 'weizenbaum': 1, '1964': 1, 'using': 3, 'almost': 1, 'thought': 1, 'emotion': 1, 'sometimes': 1, 'provided': 1, 'startlingly': 1, 'like': 1, 'interaction': 1, 'patient': 1, 'exceeded': 1, 'small': 1, 'knowledge': 1, 'base': 1, 'might': 1, 'provide': 1, 'generic': 1, 'response': 1, 'example': 2, 'responding': 1, 'head': 2, 'hurts': 2, 'say': 1, '1970s': 1, 'many': 10, 'programmers': 1, 'began': 1, 'write': 1, 'conceptual': 1, 'ontologies': 1, 'structured': 1, 'world': 5, 'understandable': 1, 'examples': 3, 'margie': 1, 'schank': 1, '1975': 1, 'sam': 1, 'cullingford': 1, '1978': 2, 'pam': 1, 'wilensky': 1, 'talespin': 1, 'meehan': 1, '1976': 1, 'qualm': 1, 'lehnert': 2, '1977': 1, 'politics': 1, 'carbonell': 1, '1979': 1, 'plot': 1, 'units': 1, '1981': 1, 'time': 2, 'chatterbots': 1, 'including': 2, 'parry': 1, 'racter': 1, 'jabberwacky': 1, 'based': 6, 'complex': 1, 'sets': 1, 'hand': 5, 'rules': 9, 'starting': 1, 'revolution': 2, 'introduction': 1, 'learning': 14, 'algorithms': 8, 'due': 3, 'steady': 1, 'increase': 2, 'computational': 1, 'power': 1, 'see': 1, 'moore': 1, 'law': 1, 'gradual': 1, 'lessening': 1, 'dominance': 1, 'chomskyan': 1, 'theories': 1, 'e': 3, 'g': 3, 'transformational': 1, 'grammar': 1, 'whose': 1, 'theoretical': 1, 'underpinnings': 1, 'discouraged': 1, 'sort': 1, 'corpus': 2, 'underlies': 1, 'approach': 1, '3': 1, 'earliest': 2, 'used': 5, 'decision': 2, 'trees': 2, 'produced': 4, 'hard': 2, 'similar': 2, 'existing': 2, 'part': 3, 'tagging': 2, 'introduced': 1, 'use': 2, 'hidden': 1, 'markov': 1, 'models': 8, 'increasingly': 3, 'focused': 3, 'make': 3, 'soft': 2, 'probabilistic': 2, 'decisions': 2, 'attaching': 2, 'valued': 2, 'weights': 2, 'features': 2, 'making': 1, 'input': 7, 'cache': 1, 'upon': 1, 'rely': 1, 'robust': 1, 'given': 3, 'unfamiliar': 1, 'especially': 2, 'contains': 2, 'errors': 1, 'common': 2, 'produce': 1, 'reliable': 2, 'results': 6, 'integrated': 1, 'larger': 3, 'comprising': 1, 'multiple': 1, 'subtasks': 2, 'notable': 1, 'early': 2, 'successes': 1, 'occurred': 1, 'field': 1, 'ibm': 1, 'successively': 1, 'complicated': 1, 'able': 1, 'take': 2, 'advantage': 2, 'multilingual': 1, 'textual': 1, 'corpora': 3, 'parliament': 1, 'canada': 1, 'european': 1, 'union': 1, 'result': 2, 'laws': 1, 'calling': 1, 'governmental': 1, 'proceedings': 1, 'official': 1, 'corresponding': 1, 'government': 1, 'depended': 1, 'specifically': 1, 'tasks': 8, 'implemented': 1, 'often': 2, 'continues': 1, 'major': 1, 'limitation': 1, 'success': 1, 'great': 1, 'deal': 1, 'gone': 1, 'methods': 2, 'effectively': 1, 'limited': 1, 'recent': 1, 'unsupervised': 1, 'semi': 1, 'supervised': 2, 'learn': 3, 'annotated': 4, 'desired': 1, 'answers': 2, 'combination': 1, 'non': 2, 'task': 2, 'difficult': 1, 'typically': 1, 'produces': 1, 'less': 1, 'accurate': 1, 'amount': 2, 'enormous': 1, 'available': 1, 'among': 1, 'things': 1, 'entire': 1, 'content': 1, 'wide': 1, 'web': 1, 'inferior': 1, 'algorithm': 1, 'low': 1, 'enough': 1, 'complexity': 1, 'practical': 1, '2010s': 1, 'representation': 1, 'deep': 3, 'neural': 3, 'network': 2, 'style': 1, 'became': 1, 'widespread': 1, 'flurry': 1, 'showing': 1, 'techniques': 2, '4': 1, '5': 1, 'achieve': 1, 'state': 1, 'art': 1, 'modeling': 2, '6': 1, 'parsing': 2, '7': 1, '8': 1, 'others': 2, 'popular': 1, 'include': 1, 'word': 2, 'embeddings': 1, 'capture': 1, 'semantic': 1, 'properties': 1, 'words': 2, 'end': 2, 'higher': 1, 'level': 1, 'question': 1, 'answering': 1, 'instead': 2, 'relying': 1, 'pipeline': 1, 'separate': 1, 'intermediate': 2, 'dependency': 1, 'areas': 1, 'shift': 1, 'entailed': 1, 'substantial': 1, 'changes': 1, 'designed': 2, 'approaches': 2, 'may': 1, 'viewed': 1, 'new': 1, 'paradigm': 2, 'distinct': 1, 'instance': 1, 'term': 1, 'nmt': 1, 'emphasizes': 1, 'fact': 1, 'directly': 1, 'sequence': 2, 'transformations': 1, 'obviating': 1, 'need': 1, 'steps': 1, 'alignment': 1, 'smt': 1, 'days': 1, 'coding': 1, 'set': 3, '9': 1, '10': 1, 'writing': 1, 'grammars': 1, 'devising': 1, 'heuristic': 1, 'stemming': 1, 'since': 1, '11': 1, '12': 1, 'mid': 1, '1990s': 1, 'relied': 1, 'heavily': 1, 'calls': 1, 'inference': 1, 'automatically': 1, 'analysis': 1, 'plural': 1, 'form': 1, 'documents': 1, 'possibly': 1, 'annotations': 1, 'typical': 1, 'different': 2, 'classes': 1, 'applied': 1, 'generated': 1, 'handwritten': 1, 'feature': 1, 'express': 1, 'relative': 1, 'certainty': 1, 'possible': 1, 'rather': 1, 'one': 1, 'producing': 1, 'model': 1, 'included': 1, 'component': 1, 'advantages': 1, 'following': 1, 'list': 1, 'commonly': 2, 'researched': 1, 'direct': 1, 'applications': 1, 'serve': 1, 'aid': 1, 'solving': 1, 'though': 1, 'closely': 1, 'intertwined': 1, 'subdivided': 1, 'categories': 1, 'convenience': 1, 'coarse': 1, 'division': 1, '2018': 1, '1': 1, 'road': 1, 'marketed': 1, 'novel': 1, 'million': 1}\n",
      "No. of words:  466\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "wordfreq = {}\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = nltk.word_tokenize(sentence) # tokenize to words\n",
    "    tokens = [i for i in tokens if i not in stopwords.words('english')] # remove stopwords\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1\n",
    "           \n",
    "print('Word frequencies: ', wordfreq)\n",
    "print('No. of words: ', len(wordfreq)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch 200 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language', 'natural', 'processing', 'machine', 'systems', 'learning', 'data', 'translation', 'many', 'statistical', 'rules', 'research', 'algorithms', 'models', 'tasks', 'real', 'input', 'however', 'based', 'results', 'world', 'hand', 'used', 'intelligence', 'human', 'speech', '1980s', 'developed', 'written', 'produced', 'annotated', 'nlp', 'linguistics', 'computer', 'information', 'large', 'generally', 'work', 'published', 'much', 'late', 'system', 'eliza', 'using', 'examples', 'due', 'e', 'g', 'part', 'increasingly', 'focused', 'make', 'given', 'larger', 'corpora', 'learn', 'deep', 'neural', 'set', 'artificial', 'computers', 'languages', 'amounts', 'frequently', 'recognition', 'found', 'turing', 'called', 'sixty', '1966', 'first', 'restricted', 'example', 'head', 'hurts', '1978', 'lehnert', 'time', 'including', 'revolution', 'increase', 'corpus', 'earliest', 'decision', 'trees', 'hard', 'similar', 'existing', 'tagging', 'use', 'soft', 'probabilistic', 'decisions', 'attaching', 'valued', 'weights', 'features', 'especially', 'contains', 'common', 'reliable', 'subtasks', 'early', 'take', 'advantage', 'result', 'often', 'methods', 'supervised', 'answers', 'non', 'task', 'amount', 'network', 'techniques', 'modeling', 'parsing', 'others', 'word', 'words', 'end', 'instead', 'intermediate', 'designed', 'approaches', 'paradigm', 'sequence', 'different', 'commonly', 'subfield', 'science', 'engineering', 'concerned', 'interactions', 'particular', 'program', 'process', 'analyze', 'challenges', 'involve', 'understanding', 'generation', 'history', 'started', '1950s', 'although', 'earlier', 'periods', '1950', 'alan', 'article', 'titled', 'computing', 'machinery', 'proposed', 'test', 'criterion', 'clarification', 'needed', 'georgetown', 'experiment', '1954', 'involved', 'fully', 'automatic', 'russian', 'sentences', 'english', 'authors', 'claimed', 'within', 'three', 'five', 'years', 'would', 'solved', 'problem', '2', 'progress', 'slower', 'alpac', 'report', 'ten', 'year', 'long', 'failed', 'fulfill', 'expectations', 'funding', 'dramatically', 'reduced', 'little', 'conducted', 'notably', 'successful', '1960s', 'shrdlu', 'working', 'blocks', 'worlds']\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "mostfreq = heapq.nlargest(200, wordfreq, key=wordfreq.get)\n",
    "\n",
    "print(mostfreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize sentences and create bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(sentences, words) = (47, 200)\n",
      "[[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentence_vectors = [] # all sentences\n",
    "for sentence in sentences:\n",
    "    tokens = nltk.word_tokenize(sentence) # break into tokens\n",
    "    sentence_vector = [] # sentence\n",
    "    for word in mostfreq:\n",
    "        if word in tokens:\n",
    "            sentence_vector.append(1)\n",
    "        else:\n",
    "            sentence_vector.append(0)\n",
    "    sentence_vectors.append(sentence_vector) # add the sentence vector to the sentences vector\n",
    "    \n",
    "BOW = np.asarray(sentence_vectors) # convert list of list to matrix\n",
    "\n",
    "print('(sentences, words) =', BOW.shape)\n",
    "print(BOW)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
